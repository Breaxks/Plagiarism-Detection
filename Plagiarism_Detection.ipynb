{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38E5KvA-N5fP"
      },
      "outputs": [],
      "source": [
        "#=========== Import Libraries ===========#\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Bidirectional, SpatialDropout1D, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== Load Dataset ===========#\n",
        "data = pd.read_csv(\"/content/sample_data/PlagiarismDataset.csv\")"
      ],
      "metadata": {
        "id": "TGQVXzRiODuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== Text Preprocessing ===========#\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):  # Check if text is not a string\n",
        "        return \"\"  # Replace non-string or NaN values with an empty string\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words(\"english\"))  # Define stopwords\n",
        "    return \" \".join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "# Apply preprocessing to the DataFrame\n",
        "data[\"source_text\"] = data[\"source_text\"].apply(preprocess_text)\n",
        "data[\"plagiarized_text\"] = data[\"plagiarized_text\"].apply(preprocess_text)\n",
        "\n",
        "# Combine source and plagiarized texts\n",
        "data[\"combined_text\"] = data[\"source_text\"] + \" \" + data[\"plagiarized_text\"]\n",
        "\n",
        "# Remove rows with missing labels\n",
        "data = data.dropna(subset=[\"label\"])\n",
        "\n",
        "# Convert labels to integers (if not already binary)\n",
        "data[\"label\"] = data[\"label\"].astype(int)\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Class distribution:\\n\", data[\"label\"].value_counts())"
      ],
      "metadata": {
        "id": "fOIgvVl4OH2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== Tokenization and Padding ===========#\n",
        "MAX_NUM_WORDS = 10000  # Vocabulary siz\n",
        "MAX_SEQUENCE_LENGTH = 100  # Max tokens per sequence\n",
        "EMBEDDING_DIM = 100  # Embedding dimension\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data[\"combined_text\"])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data[\"combined_text\"])\n",
        "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "y = data[\"label\"]\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2fegPbRBOLBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== Building & Training Model ===========#\n",
        "def build_and_train_model(model_type):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "    if model_type == \"LSTM\":\n",
        "        model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    elif model_type == \"GRU\":\n",
        "        model.add(Bidirectional(GRU(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    elif model_type == \"DNN\":\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type. Choose 'LSTM', 'GRU', or 'DNN'.\")\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping to avoid overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Train the model and save the history\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test),\n",
        "                        callbacks=[early_stopping], verbose=1)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "9eFRs_pPOOGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== Building & Training Model ===========#\n",
        "def build_and_train_model(model_type):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "    if model_type == \"LSTM\":\n",
        "        model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    elif model_type == \"GRU\":\n",
        "        model.add(Bidirectional(GRU(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    elif model_type == \"DNN\":\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type. Choose 'LSTM', 'GRU', or 'DNN'.\")\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping to avoid overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Train the model and save the history\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test),\n",
        "                        callbacks=[early_stopping], verbose=1)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "HV5BCVJOOST9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========== Building & Training Model ===========#\n",
        "def build_and_train_model(model_type):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "    if model_type == \"LSTM\":\n",
        "        model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    elif model_type == \"GRU\":\n",
        "        model.add(Bidirectional(GRU(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    elif model_type == \"DNN\":\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type. Choose 'LSTM', 'GRU', or 'DNN'.\")\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping to avoid overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Train the model and save the history\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test),\n",
        "                        callbacks=[early_stopping], verbose=1)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "e3D17d75OU8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Predicting Custom Input Text (Original vs Plagiarized Text):\n",
        "def predict_plagiarism(original_text, plagiarized_text, model, tokenizer, X_train):\n",
        "    # Preprocess the original and plagiarized texts\n",
        "    original_text_processed = preprocess_text(original_text)\n",
        "    plagiarized_text_processed = preprocess_text(plagiarized_text)\n",
        "\n",
        "    # Convert the preprocessed text into sequences\n",
        "    original_text_sequence = tokenizer.texts_to_sequences([original_text_processed])\n",
        "    plagiarized_text_sequence = tokenizer.texts_to_sequences([plagiarized_text_processed])\n",
        "\n",
        "    # Pad the sequences to ensure uniform length\n",
        "    original_text_padded = pad_sequences(original_text_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    plagiarized_text_padded = pad_sequences(plagiarized_text_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    # Make predictions using the trained model\n",
        "    original_prediction = model.predict(original_text_padded)\n",
        "    plagiarized_prediction = model.predict(plagiarized_text_padded)\n",
        "\n",
        "    # Calculate cosine similarity between the plagiarized text and the training data (original)\n",
        "    cosine_similarity_score = cosine_similarity(plagiarized_text_padded, X_train).max()\n",
        "\n",
        "    # Interpret the predictions and similarity score\n",
        "    if plagiarized_prediction[0] == 0:\n",
        "        print(\"The plagiarized text is NOT plagiarized.\")\n",
        "    else:\n",
        "        print(f\"The plagiarized text is plagiarized with a similarity score of {cosine_similarity_score * 100:.2f}%.\")\n",
        "\n",
        "    if original_prediction[0] == 0:\n",
        "        print(\"The original text is NOT plagiarized.\")\n",
        "    else:\n",
        "        print(\"The original text is plagiarized.\")  # This shouldn't typically be the case in this context.\n",
        "\n",
        "# Example Usage for Input:\n",
        "original_text = input(\"Enter the original text: \")\n",
        "plagiarized_text = input(\"Enter the plagiarized text: \")\n",
        "\n",
        "# Choose one of the trained models, e.g., \"LSTM\"\n",
        "model_to_use = trained_models[\"LSTM\"]  # Example with LSTM\n",
        "predict_plagiarism(original_text, plagiarized_text, model_to_use, tokenizer, X_train)"
      ],
      "metadata": {
        "id": "2aYfadQeOW3t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}